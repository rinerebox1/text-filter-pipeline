# Text Processing Pipeline: FineWeb Samples

This project implements a pipeline to process text samples from the FineWeb dataset. It involves loading data, filtering it using HojiChar, and then scoring it for educational content using a pre-trained transformer model.

## Pipeline Overview

The main script `main.py` orchestrates the following steps:

1.  **Load Data**: Texts are loaded from `docs/fineweb_100_samples.json`. This file contains 100 samples, each with a "text" field.
    *   The data file `docs/fineweb_100_samples.json` is pre-supplied. It can also be (re)generated by running `python サンプルで100件取得する.py` (requires `datasets` library).
2.  **Filter Text (CPU Multi-core)**: The loaded texts are processed using HojiChar (`hojichar==0.9.0`). This step normalizes the text.
    *   Output: `filtered_texts.jsonl` (each line is a JSON object: `{"text": "filtered_text_here"}`)
3.  **Score Text (GPU/CPU)**: The filtered texts are scored for educational quality using the `hotchpotch/fineweb-2-edu-japanese-classifier` model. The `gpu_scorer.py` script encapsulates the classification logic.
    *   This step uses `transformers` and `torch`. It will attempt to use a GPU if available and `torch` is installed with CUDA support; otherwise, it will fall back to CPU.
    *   Output: `scored_texts.jsonl` (each line is a JSON object: `{"text": "original_filtered_text", "is_educational": true/false, "score": float_score}`)

## Dependencies

To run the complete pipeline, you'll need the following Python libraries:

*   `hojichar==0.9.0`: For text filtering.
    ```bash
    pip install hojichar==0.9.0
    ```
*   `datasets`: Used by `サンプルで100件取得する.py` to fetch data and also by `gpu_scorer.py` (if running its `__main__` block for testing).
    ```bash
    pip install datasets
    ```
*   `pandas`: Used by `gpu_scorer.py` (if running its `__main__` block for testing) and `サンプルで100件取得する.py`.
    ```bash
    pip install pandas
    ```
*   For the scoring step (Step 3), you will need:
    *   `torch`: Core machine learning library.
    *   `transformers`: For loading and using the pre-trained model.
    *   `scikit-learn`: Used by `gpu_scorer.py` for metrics calculation in its test block.
    *   `sentencepiece`: Tokenizer often used with transformer models.
    *   `accelerate`: Utility for PyTorch.
    *   `protobuf`: Dependency for some transformer models.
    ```bash
    pip install torch transformers scikit-learn sentencepiece accelerate protobuf
    ```
    **Note on `torch` installation**: PyTorch, especially with CUDA support for GPU usage, can be very large. If you encounter installation issues due to disk space or want a CPU-only version (which will be slower for scoring), refer to the [official PyTorch installation guide](https://pytorch.org/get-started/locally/) for alternative installation commands (e.g., specifying `--index-url https://download.pytorch.org/whl/cpu` for a CPU-only version).

## Running the Pipeline

1.  **Ensure Dependencies are Installed**: See the "Dependencies" section above. For the filtering part, only `hojichar` is strictly needed by `main.py`. For data generation, `datasets` and `pandas`. For scoring, `torch`, `transformers` and others are needed.
2.  **Ensure Data File Exists**: The script expects `docs/fineweb_100_samples.json`. If it's missing, you can generate it by running:
    ```bash
    python サンプルで100件取得する.py
    ```
3.  **Run the Main Script**:
    ```bash
    python main.py
    ```
    The script will print progress messages for each step. If `torch` or `transformers` are not installed, the scoring step will be skipped with a message.

## Output Files

*   `filtered_texts.jsonl`: Contains the texts after HojiChar processing.
*   `scored_texts.jsonl`: Contains the original filtered texts along with their educational score and a boolean flag indicating if they meet the educational threshold. (This file is only generated if the scoring step runs successfully).

## Original Task Description (from previous README structure)

The initial request involved a three-step pipeline:

1.  Load data from `docs/fineweb_100_samples.json` (using only the `text` field).
2.  Use HojiChar with CPU multi-core processing to filter the texts and save them.
3.  Use `hotchpotch/fineweb-2-edu-japanese-classifier` (GPU) to assign quality scores to the filtered texts.

This functionality is now implemented in `main.py` and `gpu_scorer.py`.
