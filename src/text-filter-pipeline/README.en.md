# Text Processing Pipeline: FineWeb Samples

This project implements a Python pipeline to process text samples from the [FineWeb dataset](https://huggingface.co/datasets/HuggingFaceFW/fineweb). It involves loading data, filtering it using HojiChar, scoring it for educational content using a local pre-trained transformer model, and finally, evaluating instruction difficulty, quality, and category using the Gemini API.

## Pipeline Overview

The main script `main.py` orchestrates the following steps:

1.  **Load Data**: Texts are loaded from `src/text-filter-pipeline/data/fineweb_100_samples.json`. This file contains samples, each with a "text" field.
    *   The data file can be (re)generated by running `python src/text-filter-pipeline/サンプルで100件取得する.py` (may require `datasets` and `pandas` libraries).
    ↓
2.  **Filter Text (CPU Multi-core)**: The loaded texts are processed using the `HojiChar` library. This step normalizes the text and removes unwanted characters.
    *   Intermediate Output: `filtered_texts.jsonl` (each line is a JSON object: `{"text": "filtered_text_here"}`) saved in the same directory as `main.py`.
    ↓
3.  **Score Text (GPU/CPU)**: The filtered texts are scored for educational quality using the `hotchpotch/fineweb-2-edu-japanese-classifier` model.
    *   This step uses `transformers` and `torch`. It will attempt to use a GPU (CUDA) if available; otherwise, it will fall back to CPU.
    ↓
4.  **Gemini API Evaluations (API Call)**: For each scored text, the pipeline calls the Gemini API (`gemini-1.5-flash-latest` model by default) to perform:
    *   **Difficulty Rating**: Assesses user intent, required knowledge, and difficulty level (e.g., "easy", "hard").
    *   **Quality Rating**: Evaluates clarity, specificity, and coherence (e.g., "good", "poor").
    *   **Instruction Classification**: Assigns a primary task tag and other relevant tags (e.g., "Information seeking", "Creative writing").
    *   These evaluations utilize Gemini's structured output feature (JSON schema).
    ↓
5.  **Final Output**:
    *   Output: `final_processed_texts.jsonl` (saved in the same directory as `main.py`). Each line is a JSON object containing the original text, its educational score, and the detailed results from the Gemini evaluations.
    *   Example structure: `{"text": "...", "is_educational": true, "score": 0.95, "difficulty_rating": {"intent": "...", "difficulty": "medium"}, "quality_rating": {"explanation": "...", "quality": "average"}, "classification": {"primary_tag": "...", "other_tags": [...]}}`

## Dependencies

To run the complete pipeline, you'll need Python libraries specified in `src/text-filter-pipeline/requirements.txt`. Key dependencies include:

*   `hojichar`: For text filtering.
*   `torch`, `transformers`: For local AI model-based scoring.
*   `google-generativeai`: For accessing the Gemini API.
*   `psutil`: For system utilities (used within the text processor).

Install all dependencies using:
```bash
pip install -r src/text-filter-pipeline/requirements.txt
```

**Notes**:
*   **PyTorch (`torch`)**: Installation, especially with CUDA support for GPU, can be complex and result in large file sizes. For CPU-only or specific versions, refer to the [official PyTorch installation guide](https://pytorch.org/get-started/locally/).
*   **Gemini API Key**: The Gemini evaluation steps require a `GEMINI_API_KEY` environment variable to be set. You can obtain an API key from Google AI Studio. If the key is not set, the Gemini steps will log an error and attempt to proceed by returning error placeholders in the output.

## Running the Pipeline

1.  **Clone the Repository & Navigate**:
    ```bash
    # git clone <repository_url>
    # cd <repository_name>
    ```
    (Adapt these general commands to your project structure.) Instructions assume you are operating relative to the `src/text-filter-pipeline` directory or the project root.

2.  **Install Dependencies**:
    From your project root or within `src/text-filter-pipeline`:
    ```bash
    pip install -r src/text-filter-pipeline/requirements.txt
    ```

3.  **Prepare Data File**: The script expects `src/text-filter-pipeline/data/fineweb_100_samples.json`. If missing, generate it using:
    ```bash
    # python src/text-filter-pipeline/サンプルで100件取得する.py
    ```
    Ensure this generation script places the output in the correct `data/` subdirectory. (This script might require `datasets` and `pandas`.)

4.  **Set Gemini API Key**: Set your API key as an environment variable.
    On Linux/macOS:
    ```bash
    export GEMINI_API_KEY="YOUR_API_KEY_HERE"
    ```
    On Windows (Command Prompt):
    ```bash
    set GEMINI_API_KEY=YOUR_API_KEY_HERE
    ```
    Or for PowerShell:
    ```bash
    $env:GEMINI_API_KEY="YOUR_API_KEY_HERE"
    ```

5.  **Run the Main Script**:
    If inside `src/text-filter-pipeline`:
    ```bash
    python main.py
    ```
    If in the project root:
    ```bash
    python src/text-filter-pipeline/main.py
    ```
    The script will log progress. Missing libraries or API keys for optional steps (like scoring or Gemini evals) will result in those steps being skipped or logging errors.

## Output Files

Generated files will typically appear in the same directory as `main.py` (i.e., `src/text-filter-pipeline/`):

*   **`data/fineweb_100_samples.json`**: (Input) Expected by the pipeline.
*   **`pipeline.log`**: Runtime log for `main.py`.
*   **`filtered_texts.jsonl`**: (Intermediate) Texts after HojiChar processing.
*   **`final_processed_texts.jsonl`**: (Final Output) Contains original text, educational score, and Gemini API evaluations (`difficulty_rating`, `quality_rating`, `classification`).

## Original Task Context (Historical)

The project initially focused on a three-step pipeline:
1.  Load data.
2.  Filter with HojiChar.
3.  Score with `hotchpotch/fineweb-2-edu-japanese-classifier`.
This has been expanded with the Gemini API integration as detailed above.
