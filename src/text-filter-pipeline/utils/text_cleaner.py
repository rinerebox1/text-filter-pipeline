import os
import logging
import re
import emoji
import hojichar
from hojichar import Compose, Document
from hojichar.core.filter_interface import Filter
from hojichar.filters.deduplication import GenerateDedupLSH, LSHDeduplicator
from hojichar import document_filters
from hojichar.core.parallel import Parallel
from bs4 import BeautifulSoup

# It's good practice for utility modules to have their own logger
# or allow a logger to be passed in. For simplicity, using a local logger.
logger = logging.getLogger(__name__)

# ======================================
# Custom Filter Definitions
# ======================================
class RemoveHTMLTags(Filter):
    """HTMLã‚¿ã‚°ã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é™¤å»ã—ã€ç´”ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿"""
    def apply(self, doc: Document):
        text = BeautifulSoup(doc.text, "lxml").get_text(" ", strip=True)
        doc.text = text
        return doc

class RemoveTemplatePhrases(Filter):
    """è¨˜äº‹å†’é ­ãƒ»æœ«å°¾ã®å®šå‹ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å‰Šé™¤ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿"""
    _templates = [
        "ã“ã®è¨˜äº‹ã§ã¯",
        "çµè«–ã‹ã‚‰è¨€ã†ã¨",
        "æœ€å¾Œã¾ã§ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
        "ã„ã‹ãŒã§ã—ãŸã‹",
        "ã¾ã¨ã‚",
        "ä»¥ä¸Šã§ã™",
        "ã”è¦§ã„ãŸã ã",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
    ]
    def apply(self, doc: Document):
        t = doc.text
        for ph in self._templates:
            t = t.replace(ph, "")
        doc.text = t
        return doc

class StripEmojis(Filter):
    """çµµæ–‡å­—ã‚„æ©Ÿç¨®ä¾å­˜æ–‡å­—ã‚’å®Œå…¨ã«é™¤å»ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿"""
    def apply(self, doc: Document):
        doc.text = emoji.replace_emoji(doc.text, replace="")
        return doc

def clean_texts(texts: list[str], num_cores: int | None = None) -> list[str]:
    """
    Cleans a list of text strings using HojiChar.

    Args:
        texts: A list of raw text strings.
        num_cores: The number of CPU cores to use for parallel processing.
                   Defaults to os.cpu_count().

    Returns:
        A list of cleaned text strings.
    """
    if not texts:
        logger.info("No texts provided to clean_texts function.")
        return []

    logger.info(f"Starting text cleaning with HojiChar for {len(texts)} texts.")

    # Create comprehensive cleaning pipeline based on the reference code
    cleaner = Compose([
        # --- è¨€èªãƒ•ã‚£ãƒ«ã‚¿ ---
        document_filters.AcceptJapanese(p=0.5),             # FastTextãƒ™ãƒ¼ã‚¹ã§æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’é€šéã•ã›ã‚‹(é–¾å€¤0.5)
        document_filters.ExampleHojiChar(),

        # --- å‰å‡¦ç†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ç³»ï¼‰---
        RemoveHTMLTags(),                                   # HTMLã‚¿ã‚°å‰Šé™¤
        RemoveTemplatePhrases(),                            # å®šå‹æ–‡å‰Šé™¤
        StripEmojis(),                                      # çµµæ–‡å­—å‰Šé™¤
        document_filters.DocumentNormalizer(),              # Unicode NFKC æ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’ãªã©ï¼‰

        # --- å½¢æ…‹ï¼†é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ ---
        document_filters.DocumentLengthFilter(              # æ¥µç«¯ã«çŸ­ã„/é•·ã„æ–‡æ›¸ã‚’ç ´æ£„
            min_doc_len=50, max_doc_len=20000),
        document_filters.DiscardTooShortLines(              # çŸ­ã™ãã‚‹è¡ŒãŒå¤šã™ãã‚Œã°ç ´æ£„
            threshold=0.5, min_length=5),
        document_filters.DiscardTooManyNouns(               # åè©ã°ã‹ã‚Šã®ç¾…åˆ—ã‚’ç ´æ£„ï¼ˆè¦ fugashiï¼‰
            max_noun_ratio=0.8),
        document_filters.CharRepetitionRatioFilter(),       # åŒä¸€æ–‡å­—ã®é€£ç¶šãŒå¤šã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        document_filters.WordRepetitionRatioFilter(),       # åŒã˜å˜èªãƒ»èªå¥ã®ç¹°ã‚Šè¿”ã—ãŒå¤šã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        document_filters.SingleCharacterRepetitionFilter(), # å˜ä¸€æ–‡å­—ã®é€£ç¶šï¼ˆä¾‹ï¼šã€Œã‚ã‚ã‚ã‚ã€ï¼‰ãŒå¤šã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        document_filters.DiscardTooManyEndingEllipsis(),    # ã€Œâ€¦â€¦ã€ãªã©ã€æœ«å°¾ã®çœç•¥è¨˜å·ãŒå¤šã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»

        # --- ä¸é©åˆ‡è¡¨ç¾ãƒ»ã‚¹ãƒ‘ãƒ æ¤œçŸ¥ ---
        document_filters.DiscardAds(),                      # åºƒå‘Šã‚¹ãƒ‘ãƒ åˆ¤å®š
        document_filters.DiscardAdultContentJa(),           # ã‚¢ãƒ€ãƒ«ãƒˆè¡¨ç¾
        document_filters.DiscardViolenceContentJa(),        # æš´åŠ›è¡¨ç¾
        document_filters.DiscardDiscriminationContentJa(),  # å·®åˆ¥è¡¨ç¾
        document_filters.DiscardTooManySpecialToken(        # è¨˜å·ã‚„çµµæ–‡å­—ã°ã‹ã‚Šã®æ–‡æ›¸ã‚’ç ´æ£„
            threshold=0.4),

        # --- é‡è¤‡é™¤å» ---
        GenerateDedupLSH(),                                  # LSH (å±€æ‰€æ„Ÿåº¦ãƒãƒƒã‚·ãƒ¥) ã‚’ä½¿ã£ã¦é‡è¤‡æ¤œå‡ºæƒ…å ±ã‚’Documentã«ä»˜ä¸
        LSHDeduplicator(),                                   # æ—¢å‡ºãƒãƒƒã‚·ãƒ¥ã¨ã®é‡è¤‡åˆ¤å®šãƒ»ç ´æ£„
    ])

    input_documents = [hojichar.Document(text) for text in texts]

    if num_cores is None:
        num_cores = os.cpu_count() or 1  # Default to 1 core if os.cpu_count() is None

    logger.info(f"Using {num_cores} CPU cores for HojiChar processing.")

    cleaned_texts_content = []
    rejected_count = 0
    
    # Initialize Parallel with the cleaner and number of jobs
    with Parallel(cleaner, num_jobs=num_cores) as pfilter:
        # pfilter.imap_apply returns an iterator of processed Document objects
        for i, doc in enumerate(pfilter.imap_apply(input_documents)):
            if not doc.is_rejected:
                cleaned_texts_content.append(doc.text)
            else:
                rejected_count += 1
            
            if (i + 1) % 100 == 0: # Log progress every 100 texts
                logger.info(f"HojiChar processed {i+1}/{len(texts)} texts in clean_texts... "
                           f"(Accepted: {len(cleaned_texts_content)}, Rejected: {rejected_count})")

    logger.info(f"HojiChar cleaning complete in clean_texts. "
               f"Processed {len(texts)} texts -> "
               f"Accepted: {len(cleaned_texts_content)}, Rejected: {rejected_count}")
    return cleaned_texts_content


if __name__ == '__main__':
    # Example usage:
    # This part is for testing the module directly.
    # It requires HojiChar to be installed.
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    sample_texts = [
        "ã“ã‚Œã¯æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã§ã€ååˆ†ãªé•·ã•ãŒã‚ã‚Šã¾ã™ã€‚ãƒ†ã‚¹ãƒˆã®ãŸã‚ã«ä½œæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚",
        "ã“ã‚Œã¯ã€€ï¼’ç•ªç›®ã®ã€€ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã€€ã‚¹ãƒšãƒ¼ã‚¹ãŒå¤šã„ã§ã™ã€‚å…¨è§’æ•°å­—ã‚„å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚æ­£è¦åŒ–ã®ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã—ã¾ã™ã€‚",
        "This is the third text with English words. It should be filtered out by AcceptJapanese filter.",
        "<p>HTMLã‚¿ã‚°ãŒå«ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚</p><div>ã“ã‚Œã‚‰ã®ã‚¿ã‚°ã¯é™¤å»ã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚</div>BeautifulSoupã«ã‚ˆã£ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚",
        "ã“ã®è¨˜äº‹ã§ã¯ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ¬ãƒ¼ã‚ºã®ãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã€‚å®šå‹æ–‡ãŒé™¤å»ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚æœ€å¾Œã¾ã§ã”è¦§ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
        "çµµæ–‡å­—ãƒ†ã‚¹ãƒˆğŸ˜€ğŸ‰âœ¨ ã“ã‚Œã‚‰ã®çµµæ–‡å­—ã¯é™¤å»ã•ã‚Œã‚‹ã¯ãšã§ã™ ğŸš€ğŸ’¯",
        "é«™å³¶å±‹ã¨é«™å³¶å±‹ã§ã®è²·ã„ç‰©ä½“é¨“ã«ã¤ã„ã¦ã€‚æ­£è¦åŒ–ã«ã‚ˆã‚Šé«™ãŒé«˜ã«å¤‰æ›ã•ã‚Œã‚‹ã‹ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚",
        "çŸ­ã„", # This should be filtered out by DocumentLengthFilter
        "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa", # This should be filtered out by SingleCharacterRepetitionFilter
        "é‡è¤‡é‡è¤‡é‡è¤‡é‡è¤‡é‡è¤‡é‡è¤‡é‡è¤‡é‡è¤‡é‡è¤‡é‡è¤‡", # This should be filtered out by WordRepetitionRatioFilter
    ]

    logger.info(f"Original texts count: {len(sample_texts)}")
    for i, text in enumerate(sample_texts):
        logger.info(f"Text {i+1}: {text[:50]}...")

    # Using default number of cores
    cleaned = clean_texts(sample_texts)

    logger.info(f"Cleaned texts count: {len(cleaned)}")
    for i, text in enumerate(cleaned):
        logger.info(f"Cleaned {i+1}: {text[:100]}...")

    # Note: Due to filtering, len(cleaned) may be less than len(sample_texts)
    logger.info(f"Filtering ratio: {len(cleaned)}/{len(sample_texts)} = {len(cleaned)/len(sample_texts)*100:.1f}% passed")

    logger.info("Test run of text_cleaner.py complete.")

